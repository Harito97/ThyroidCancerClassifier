{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cài đặt các gói cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx onnxscript onnxruntime\n",
    "# pip install onnx onnxscript để có thể chuyển model từ .pth hoặc .pt sang .onnx\n",
    "# pip install onnxruntime để có môi trường chạy model định dạng tệp .onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng model hoặc load vào model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file_path = '.../model.pt'\n",
    "model_file_path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đây là ví dụ cách load vào model H97_ResNet đã được train và lưu vào file có đường dẫn model_file_path\n",
    "from src.model.classifier.H97 import H97_ResNet\n",
    "import torch\n",
    "\n",
    "torch_model = H97_ResNet()\n",
    "torch_model.load_state_dict(torch.load(model_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model từ dạng của PyTorch sang format của ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lưu model ở format của ONNX thành file .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_destination_path = '.../my_image_classifier.onnx'\n",
    "model_destination_path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load lại model từ file .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(model_destination_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trực quan kiến trúc model (lưu ở định dạng tệp .onnx) với [Netron](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truy cập trang web [sau đây](https://netron.app/). Sau đó chọn tệp tin .onnx và xem hình ảnh trực quan ra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chạy 1 model .onnx với môi trường onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "torch_input = torch.randn(\n",
    "    1, 1, 32, 32\n",
    ")  # sửa lại kích thước input cho phù hợp với model của mình\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "# Tạo 1 session để suy luận model\n",
    "# tại đường dẫn model_destination_path\n",
    "# với môi trường CPUExecutionProvider hoặc CUDAExecutionProvider\n",
    "# Kiểm tra xem có GPU hỗ trợ ONNX Runtime không\n",
    "if \"CUDAExecutionProvider\" in onnxruntime.get_available_providers():\n",
    "    providers = [\"CUDAExecutionProvider\"]\n",
    "else:\n",
    "    providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "# Tạo phiên suy luận sử dụng thiết bị phù hợp\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"./model_destination_path.onnx\", providers=providers\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Định nghĩa một hàm để chuyển tensor PyTorch sang mảng NumPy.\n",
    "    Điều này cần thiết vì ONNX Runtime sử dụng mảng NumPy làm định dạng dữ liệu đầu vào.\n",
    "    Nếu đang sử dụng GPU, tensor sẽ được chuyển về CPU trước khi chuyển sang NumPy.\n",
    "    \"\"\"\n",
    "    if \"CUDAExecutionProvider\" in onnxruntime.get_available_providers():\n",
    "        tensor = tensor.cuda()\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "# Tạo một từ điển của dữ liệu đầu vào cho ONNX Runtime,\n",
    "# chuyển đổi tensor đầu vào PyTorch sang mảng NumPy và\n",
    "# gán chúng với tên của các đầu vào mô hình như được xác định trong phiên suy luận ONNX.\n",
    "onnxruntime_input = {\n",
    "    k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)\n",
    "}\n",
    "\n",
    "# Thực hiện suy luận với dữ liệu đầu vào đã được chuẩn bị,\n",
    "# None ở đây chỉ ra rằng chúng ta muốn lấy tất cả đầu ra từ mô hình.\n",
    "# Kết quả suy luận được lưu trong onnxruntime_outputs.\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So sánh kết quả chạy bởi model PyTorch với model .onnx có giống nhau không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_outputs = torch_model(torch_input)\n",
    "torch_outputs = onnx_program.adapt_torch_outputs_to_onnx(torch_outputs)\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
    "\n",
    "print(\"PyTorch and ONNX Runtime output matched!\")\n",
    "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
    "print(f\"Sample output: {onnxruntime_outputs}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
